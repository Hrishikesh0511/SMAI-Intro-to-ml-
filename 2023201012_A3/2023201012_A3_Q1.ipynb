{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZHhGdCgAEGa",
        "outputId": "fd0ea970-3c8b-4805-e574-4e408e021726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzvWTa3LAQSD"
      },
      "source": [
        "# 1. Multilayer Perceptron (MLP):\n",
        "* Develop a Multilayer Perceptron (MLP) model with an appropriate architecture (number\n",
        "of layers, hidden units, etc.).\n",
        "* Train the MLP model on the CIFAR-10 dataset.\n",
        "* Evaluate its performance on the test set using metrics like accuracy and loss. Analyze\n",
        "the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCSah_e9DWMr",
        "outputId": "49544247-6ada-4c18-f75a-5633308cd083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,   200] loss: 1.746\n",
            "[2,   200] loss: 1.440\n",
            "[3,   200] loss: 1.302\n",
            "[4,   200] loss: 1.202\n",
            "[5,   200] loss: 1.108\n",
            "[6,   200] loss: 1.027\n",
            "[7,   200] loss: 0.953\n",
            "[8,   200] loss: 0.872\n",
            "[9,   200] loss: 0.795\n",
            "[10,   200] loss: 0.717\n",
            "[11,   200] loss: 0.671\n",
            "[12,   200] loss: 0.591\n",
            "[13,   200] loss: 0.526\n",
            "[14,   200] loss: 0.480\n",
            "[15,   200] loss: 0.431\n",
            "[16,   200] loss: 0.382\n",
            "[17,   200] loss: 0.369\n",
            "[18,   200] loss: 0.335\n",
            "[19,   200] loss: 0.305\n",
            "[20,   200] loss: 0.308\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 53 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs, labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:  # Print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images, labels\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation:\n",
        "* Since MLP models may not perform well due to their inability to effectively capture spatial dependencies and patterns present in images, as they lack the ability to handle the high-dimensional input data and spatial information effectively the accuracy was some what on lower side."
      ],
      "metadata": {
        "id": "_txDPC_0wl9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "* Design a simple Convolutional Neural Network (CNN) architecture with convolutional\n",
        "layers, pooling layers, and fully connected layers.\n",
        "* Train the CNN model on the CIFAR-10 dataset.\n",
        "* Evaluate its performance on the test set using the same metrics as the MLP. Critically\n",
        "compare CNN's performance to the MLP's."
      ],
      "metadata": {
        "id": "FYl5ntBFxPy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BabVu3GlHm2y",
        "outputId": "79387d0f-6f4f-4c97-8f47-7e5138c77c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,   200] loss: 1.620\n",
            "[2,   200] loss: 1.055\n",
            "[3,   200] loss: 0.828\n",
            "[4,   200] loss: 0.677\n",
            "[5,   200] loss: 0.555\n",
            "[6,   200] loss: 0.459\n",
            "[7,   200] loss: 0.356\n",
            "[8,   200] loss: 0.264\n",
            "[9,   200] loss: 0.199\n",
            "[10,   200] loss: 0.136\n",
            "[11,   200] loss: 0.099\n",
            "[12,   200] loss: 0.073\n",
            "[13,   200] loss: 0.070\n",
            "[14,   200] loss: 0.048\n",
            "[15,   200] loss: 0.053\n",
            "[16,   200] loss: 0.044\n",
            "[17,   200] loss: 0.052\n",
            "[18,   200] loss: 0.060\n",
            "[19,   200] loss: 0.034\n",
            "[20,   200] loss: 0.032\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 76 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # channels = 3, input = 32*32, kernel_size = 3*3, padding =1\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        # (w-f+2d)/s + 1 (Output size = (32 - 3 + 2*1)/1 + 1 = 32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:  # Print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation and Analysis\n",
        "* Since CNN has the to capture spatial hierarchies and local patterns through convolutional layers and reduce the number of parameters by sharing weights, making them more suitable for handling image data.\n",
        "* So it better performed than regular MLP."
      ],
      "metadata": {
        "id": "tQh7SVbXxsdX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n4jgEfoGzf4"
      },
      "source": [
        "# Transfer Learning with VGG\n",
        "* Utilize a pre-trained VGG model (such as VGG16 or VGG19) available in common deep\n",
        "learning libraries.\n",
        "* Adapt the pre-trained VGG model for the CIFAR-10 or CIFAR-100 classification task.\n",
        "* Evaluate its performance on the test set. Analyze the benefits of transfer learning\n",
        "compared to your MLP and CNN models trained from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI8QxFV4FkFt",
        "outputId": "9823069c-a07f-48e4-f286-b005124e1b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n",
            "Epoch 1/5: 100%|██████████| 782/782 [12:40<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 782/782 [12:39<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 782/782 [12:37<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 782/782 [12:36<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 782/782 [12:37<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 157/157 [01:04<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),  # to reduce overfitting\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load pre-trained VGG model\n",
        "vgg_model = torch.hub.load(\"pytorch/vision\", \"vgg16\", weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Modify the last layer for CIFAR-10 classification\n",
        "num_features = vgg_model.fc.in_features\n",
        "vgg_model.fc = nn.Linear(num_features, 10)  # CIFAR-10 has 10 classes\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(vgg_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg_model.to(device)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    vgg_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(trainloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = vgg_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(trainset)\n",
        "    print(f'Training Loss: {epoch_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "vgg_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(testloader, desc='Testing'):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = vgg_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print('Test Accuracy:',accuracy*100,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxD96wZQKLAs",
        "outputId": "dc2e62a4-fc85-460b-a67d-1c6dbf2c2c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 157/157 [01:05<00:00,  2.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.57 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation and analysis:\n",
        "* Using VGG model for transfer learning improved performance by leveraging pre-trained weights learned from large-scale image datasets. This allowed the model to benefit from features learned by the VGG model, which are transferable to the new task, thereby accelerating convergence, reducing the need for large amounts of labeled data, and resulted in higher accuracy compared to training from scratch."
      ],
      "metadata": {
        "id": "TKZgHypqyYwV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}